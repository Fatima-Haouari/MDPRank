{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling PRF in pyterier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.8.1 has loaded Terrier 5.6 (built by craigmacdonald on 2021-09-17 13:27)\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as gensim_api\n",
    "import string\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import pyterrier as pt\n",
    "from pyterrier.measures import RR, R, Rprec, P, MAP\n",
    "import global_variables as gb\n",
    "\n",
    "if not pt.started():\n",
    "    print(\"Enabling PRF in pyterier\")\n",
    "    # In this lab, we need to specify that we start PyTerrier with PRF enabled\n",
    "    pt.init(boot_packages=[\"com.github.terrierteam:terrier-prf:-SNAPSHOT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_emoji_smileys(text):\n",
    "    try:\n",
    "        # UCS-4\n",
    "        EMOJIS_PATTERN = re.compile(\n",
    "            u\"([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])\"\n",
    "        )\n",
    "    except re.error:\n",
    "        # UCS-2\n",
    "        EMOJIS_PATTERN = re.compile(\n",
    "            u\"([\\u2600-\\u27BF])|([\\uD83C][\\uDF00-\\uDFFF])|([\\uD83D][\\uDC00-\\uDE4F])|([\\uD83D][\\uDE80-\\uDEFF])\"\n",
    "        )\n",
    "\n",
    "    SMILEYS_PATTERN = re.compile(r\"(\\s?:X|:|;|=)(?:-)?(?:\\)+|\\(|O|D|P|S|\\\\|\\/\\s){1,}\", re.IGNORECASE)\n",
    "\n",
    "    text = SMILEYS_PATTERN.sub(r\"\", text)\n",
    "    text = EMOJIS_PATTERN.sub(r\"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)  # remove urls\n",
    "    text = re.sub(r\"RT \", \" \", text)  # remove rt\n",
    "    text = re.sub(r\"@[\\w]*\", \" \", text)  # remove handles\n",
    "    text = re.sub(r\"[\\.\\,\\#_\\|\\:\\?\\?\\/\\=]\", \" \", text) # remove special characters\n",
    "    text = re.sub(r\"\\t\", \" \", text)  # remove tabs\n",
    "    text = re.sub(r\"\\n\", \" \", text)  # remove line jump\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # remove extra white space\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# read file based on its extension (tsv or xlsx)\n",
    "def read_file(input_file, sep=\"\\t\", names = \"\"):\n",
    "    if input_file.endswith(\".xlsx\"):\n",
    "        df = pd.read_excel(input_file)\n",
    "    else:\n",
    "        if names != \"\":\n",
    "            df = pd.read_csv(input_file, sep=sep, names=names,encoding=\"utf-8\")\n",
    "        else:\n",
    "            df = pd.read_csv(input_file, sep=sep,encoding=\"utf-8\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove_twitter_25 = gensim_api.load(\"glove-twitter-25\")\n",
    "# model_glove_twitter_100 = gensim_api.load(\"glove-twitter-100\")\n",
    "# model_glove_google_300 = gensim_api.load(\"word2vec-google-news-300\")\n",
    "# word2vec_model = model_glove_google_300\n",
    "word2vec_model = model_glove_twitter_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data was formatted and saved successfully in  ./pre_prosess/VCR22/en-2022-validation-formatted.tsv\n",
      "Data was formatted and saved successfully in  ./pre_prosess/VCR22/en-2022-test-formatted.tsv\n",
      "Data was formatted and saved successfully in  ./pre_prosess/VCR22/en-2022-train-formatted.tsv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_mean_vector(text, w2v_model):\n",
    "    tokens = [t for t in text.lower().split() if t in w2v_model.key_to_index]\n",
    "    if len(tokens) == 0:\n",
    "        print(\"Error: no tokens were found in the model vocabulary for this text \", text)\n",
    "        return {}\n",
    "    \n",
    "    mean_vector = w2v_model.get_mean_vector(tokens, pre_normalize=True)\n",
    "    return mean_vector, tokens\n",
    "\n",
    "\n",
    "def get_query_docs_features(w2v_model, query, query_id, docs, doc_ids, labels):\n",
    "\n",
    "    SEP_TOKEN = 256 \n",
    "    query_mean_vector, query_tokens = get_mean_vector(query, w2v_model)\n",
    "    df_query = pd.DataFrame(columns=gb.input_column_names)\n",
    "\n",
    "    for i in range(len(docs)):\n",
    "        df_one = pd.DataFrame(columns=gb.input_column_names)\n",
    "        all_tokens_vector = []\n",
    "        doc = docs[i]\n",
    "        doc_mean_vector, doc_tokens = get_mean_vector(doc, w2v_model)\n",
    "        cosine_sim =w2v_model.n_similarity(query_tokens, doc_tokens)\n",
    "        \n",
    "        # query + doc = 50 tokens\n",
    "        all_tokens_vector \n",
    "def get_mean_vector(text, w2v_model):\n",
    "    tokens = [t for t in text.lower().split() if t in w2v_model.key_to_index]\n",
    "    if len(tokens) == 0:\n",
    "        print(\"Error: no tokens were found in the model vocabulary for this text \", text)\n",
    "        return {}\n",
    "    \n",
    "    mean_vector = w2v_model.get_mean_vector(tokens, pre_normalize=True)\n",
    "    return mean_vector, tokens\n",
    "\n",
    "\n",
    "def get_query_docs_features(w2v_model, query, query_id, docs, doc_ids, labels):\n",
    "\n",
    "    SEP_TOKEN = 256 \n",
    "    query_mean_vector, query_tokens = get_mean_vector(query, w2v_model)\n",
    "    df_query = pd.DataFrame(columns=gb.input_column_names)\n",
    "\n",
    "    for i in range(len(docs)):\n",
    "        df_one = pd.DataFrame(columns=gb.input_column_names)\n",
    "        doc = docs[i]\n",
    "        doc_mean_vector, doc_tokens = get_mean_vector(doc, w2v_model)\n",
    "        cosine_sim =w2v_model.n_similarity(query_tokens, doc_tokens)\n",
    "        \n",
    "        # query + doc = 50 tokens\n",
    "        all_tokens_vector = np.concatenate((query_mean_vector, doc_mean_vector)).tolist()\n",
    "\n",
    "        # sep + query + sep + doc + sep + cosine-sim + sep = 55\n",
    "        # all_tokens_vector.append(SEP_TOKEN)\n",
    "        # all_tokens_vector.append(query_mean_vector)\n",
    "        # all_tokens_vector.append(SEP_TOKEN)\n",
    "        # all_tokens_vector.append(doc_mean_vector)\n",
    "        # all_tokens_vector.append(SEP_TOKEN)\n",
    "        # all_tokens_vector.append(cosine_sim)\n",
    "        # all_tokens_vector.append(SEP_TOKEN)\n",
    "\n",
    "        one_row = {\n",
    "            gb.QUERY_ID : query_id,\n",
    "            gb.DOCID : doc_ids[i],\n",
    "            gb.FEATURE : all_tokens_vector,\n",
    "            gb.FLAG : labels[i],\n",
    "        }\n",
    "        \n",
    "\n",
    "        df_query = df_query.append(one_row, ignore_index=True)\n",
    "    \n",
    "\n",
    "    return df_query\n",
    "\n",
    "\n",
    "\n",
    "def create_data(input_data, output_path, w2v_model=model_glove_twitter_25):\n",
    "    # columns = 'tweet_id\ttweet_text\tvclaim_id\tvclaim\tlabel\trank\tscore\ttitle\tlexical_similarity\tsemantic_similarity'\n",
    "\n",
    "    df_input = read_file(input_data)\n",
    "    df_res = pd.DataFrame(columns=gb.input_column_names)\n",
    "\n",
    "    for query_id in df_input['tweet_id'].unique():\n",
    "        df_query = df_input[df_input['tweet_id'] == query_id]\n",
    "        query = df_query['tweet_text'].values[0]\n",
    "        docs = df_query['vclaim'].values\n",
    "        doc_ids = df_query['vclaim_id'].values\n",
    "        labels = df_query['label'].values\n",
    "\n",
    "        df_formatted = get_query_docs_features(w2v_model, query, query_id, docs, doc_ids, labels)\n",
    "        df_res = df_res.append(df_formatted, ignore_index=True)\n",
    "\n",
    "    \n",
    "    df_res[gb.FEATURE] = df_res[gb.FEATURE].astype(str).str.strip('[|]') # to remove brackets before writing to csv \n",
    "    df_res.to_csv(output_path, sep='\\t',  header=False,  index=False)\n",
    "    print(\"Data was formatted and saved successfully in \", output_path)\n",
    "\n",
    "\n",
    "\n",
    "train_set = './pre_prosess/VCR22/en-clef2022-train_set_top_10.tsv'\n",
    "val_set = './pre_prosess/VCR22/en-clef2022-mono_bert_dev_set_top_10.tsv'\n",
    "test_set = './pre_prosess/VCR22/mono_bert_test_set_top_10.tsv'\n",
    "\n",
    "train_output = './pre_prosess/VCR22/en-2022-train-formatted.tsv'\n",
    "val_output = './pre_prosess/VCR22/en-2022-validation-formatted.tsv'\n",
    "test_output = './pre_prosess/VCR22/en-2022-test-formatted.tsv'\n",
    "\n",
    "create_data(val_set, val_output)\n",
    "create_data(test_set, test_output)\n",
    "create_data(train_set, train_output)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('WM-MDPRank')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db60f26d632afa442bed552bbc32d3646fbd2fe0ae6ebf89be6a58caf487b008"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
